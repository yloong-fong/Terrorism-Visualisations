dtm_left<-DocumentTermMatrix(tm_left)
library(tidytext)
sotu_td_left <- tidy(dtm_left)
sotu_tf_idf_left <-  sotu_td_left %>%
bind_tf_idf(term, document, count) %>%
arrange(desc(tf_idf))
# top 300 words using count
top_words_left <- sotu_tf_idf_left %>%
dplyr::group_by(term) %>%
dplyr::summarise(n = sum(count)) %>%
dplyr::top_n(n = 300, wt = n)  %>%
dplyr::ungroup() %>%
dplyr::mutate(term = reorder(term, n))
# using text analysis - religious
library(tm)
relig_text <- data.frame(doc_id=relig$DOM_I, text=relig$PHIL, stringsAsFactors = FALSE)
df_source_relig <- DataframeSource(relig_text)
tm_relig <- VCorpus(df_source_relig)
tm2_relig <- tm_map(tm_relig, content_transformer(tolower))
tm2_relig <- tm_map(tm2_relig, removePunctuation)
tm2_relig <- tm_map(tm2_relig, removeWords, c(stopwords("en")))
tm2_relig <- tm_map(tm2_relig, removeWords, my_custom_stopwords)
tm2_relig <- tm_map(tm2_relig, removeNumbers)
removeNumPunct <- function(x){gsub("[^[:alpha:][:space:]]*", "", x)}
tm2_relig <- tm_map(tm2_relig, content_transformer(removeNumPunct))
tm2_relig <- tm_map(tm2_relig, stripWhitespace)
# stemming
library(SnowballC)
# Stem all words
tm_stemmed_relig <- tm_map(tm2_relig, stemDocument)
# Stem completion
stemCompletion2 <- function(x, dictionary) {
x <- unlist(strsplit(as.character(x), " "))
x <- x[x != ""]
x <- stemCompletion(x, dictionary=dictionary)
x <- paste(x, sep="", collapse=" ")
PlainTextDocument(stripWhitespace(x))
}
tm_all_relig <- lapply(tm_stemmed_relig, stemCompletion2,
dictionary=tm2_relig)
tm_relig <- VCorpus(VectorSource(tm_all_relig))
library(tm)
dtm_relig<-DocumentTermMatrix(tm_relig)
library(tidytext)
sotu_td_relig <- tidy(dtm_relig)
sotu_tf_idf_relig <-  sotu_td_relig %>%
bind_tf_idf(term, document, count) %>%
arrange(desc(tf_idf))
# top 300 words using count
top_words_relig <- sotu_tf_idf_relig %>%
dplyr::group_by(term) %>%
dplyr::summarise(n = sum(count)) %>%
dplyr::top_n(n = 300, wt = n)  %>%
dplyr::ungroup() %>%
dplyr::mutate(term = reorder(term, n))
# using text analysis - ethno
library(tm)
ethno_text <- data.frame(doc_id=ethno$DOM_I, text=ethno$PHIL, stringsAsFactors = FALSE)
df_source_ethno <- DataframeSource(ethno_text)
tm_ethno <- VCorpus(df_source_ethno)
tm2_ethno <- tm_map(tm_ethno, content_transformer(tolower))
tm2_ethno <- tm_map(tm2_ethno, removePunctuation)
tm2_ethno <- tm_map(tm2_ethno, removeWords, c(stopwords("en")))
tm2_ethno <- tm_map(tm2_ethno, removeWords, my_custom_stopwords)
tm2_ethno <- tm_map(tm2_ethno, removeNumbers)
removeNumPunct <- function(x){gsub("[^[:alpha:][:space:]]*", "", x)}
tm2_ethno <- tm_map(tm2_ethno, content_transformer(removeNumPunct))
tm2_ethno <- tm_map(tm2_ethno, stripWhitespace)
# stemming
library(SnowballC)
# Stem all words
tm_stemmed_ethno <- tm_map(tm2_ethno, stemDocument)
# Stem completion
stemCompletion2 <- function(x, dictionary) {
x <- unlist(strsplit(as.character(x), " "))
x <- x[x != ""]
x <- stemCompletion(x, dictionary=dictionary)
x <- paste(x, sep="", collapse=" ")
PlainTextDocument(stripWhitespace(x))
}
tm_all_ethno <- lapply(tm_stemmed_ethno, stemCompletion2,
dictionary=tm2_ethno)
tm_ethno <- VCorpus(VectorSource(tm_all_ethno))
library(tm)
dtm_ethno<-DocumentTermMatrix(tm_ethno)
library(tidytext)
sotu_td_ethno <- tidy(dtm_ethno)
sotu_tf_idf_ethno <-  sotu_td_ethno %>%
bind_tf_idf(term, document, count) %>%
arrange(desc(tf_idf))
# top 300 words using count
top_words_ethno <- sotu_tf_idf_ethno %>%
dplyr::group_by(term) %>%
dplyr::summarise(n = sum(count)) %>%
dplyr::top_n(n = 300, wt = n)  %>%
dplyr::ungroup() %>%
dplyr::mutate(term = reorder(term, n))
# using text analysis - single
library(tm)
single_text <- data.frame(doc_id=single$DOM_I, text=single$PHIL, stringsAsFactors = FALSE)
df_source_single <- DataframeSource(single_text)
tm_single <- VCorpus(df_source_single)
tm2_single <- tm_map(tm_single, content_transformer(tolower))
tm2_single <- tm_map(tm2_single, removePunctuation)
tm2_single <- tm_map(tm2_single, removeWords, c(stopwords("en")))
tm2_single <- tm_map(tm2_single, removeWords, my_custom_stopwords)
tm2_single <- tm_map(tm2_single, removeNumbers)
removeNumPunct <- function(x){gsub("[^[:alpha:][:space:]]*", "", x)}
tm2_single <- tm_map(tm2_single, content_transformer(removeNumPunct))
tm2_single <- tm_map(tm2_single, stripWhitespace)
# stemming
library(SnowballC)
# Stem all words
tm_stemmed_single <- tm_map(tm2_single, stemDocument)
# Stem completion
stemCompletion2 <- function(x, dictionary) {
x <- unlist(strsplit(as.character(x), " "))
x <- x[x != ""]
x <- stemCompletion(x, dictionary=dictionary)
x <- paste(x, sep="", collapse=" ")
PlainTextDocument(stripWhitespace(x))
}
tm_all_single <- lapply(tm_stemmed_single, stemCompletion2,
dictionary=tm2_single)
tm_single <- VCorpus(VectorSource(tm_all_single))
library(tm)
dtm_single<-DocumentTermMatrix(tm_single)
library(tidytext)
sotu_td_single <- tidy(dtm_single)
sotu_tf_idf_single <-  sotu_td_single %>%
bind_tf_idf(term, document, count) %>%
arrange(desc(tf_idf))
# top 300 words using count
top_words_single <- sotu_tf_idf_single %>%
dplyr::group_by(term) %>%
dplyr::summarise(n = sum(count)) %>%
dplyr::top_n(n = 300, wt = n)  %>%
dplyr::ungroup() %>%
dplyr::mutate(term = reorder(term, n))
top_words_right <- merge(c("Right-wing"), top_words_right)
top_words_left <- merge(c("Left-wing"), top_words_left)
top_words_relig <- merge(c("Religious"), top_words_relig)
top_words_ethno <- merge(c("Ethno-nationalist"), top_words_ethno)
top_words_single <- merge(c("Single issue"), top_words_single)
allwords <- rbind(top_words_right, top_words_left)
allwords <- rbind(allwords, top_words_relig)
allwords <- rbind(allwords, top_words_ethno)
allwords <- rbind(allwords, top_words_single)
names(allwords)[1] <- "ideology"
blue_gradient <- brewer.pal(9, "Blues")
blue_gradient <- blue_gradient[-(7:9)]
red_gradient <- brewer.pal(9, "Reds")
red_gradient <- red_gradient[-(7:9)]
green_gradient <- brewer.pal(9, "Greens")
green_gradient <- green_gradient[-(7:9)]
purple_gradient <- brewer.pal(9, "Purples")
purple_gradient <- purple_gradient[-(7:9)]
orange_gradient <- brewer.pal(9, "Oranges")
orange_gradient <- orange_gradient[-(7:9)]
blue_gradient <- merge(c("Right-wing"), blue_gradient)
red_gradient <- merge(c("Left-wing"), red_gradient)
green_gradient <- merge(c("Religious"), green_gradient)
purple_gradient <- merge(c("Ethno-nationalist"), purple_gradient)
orange_gradient <- merge(c("Single issue"), orange_gradient)
allcolours <- rbind(blue_gradient, red_gradient)
allcolours <- rbind(allcolours, green_gradient)
allcolours <- rbind(allcolours, purple_gradient)
allcolours <- rbind(allcolours, orange_gradient)
names(allcolours)[1] <- "ideology"
names(allcolours)[2] <- "colours"
require(shiny)
ui <- fluidPage(
# Application title
titlePanel("Word Cloud"),
sidebarLayout(
# Sidebar with a slider and selection inputs
sidebarPanel(
selectInput("ideology", "Choose an ideology:",
choices = c("Right-wing", "Left-wing", "Religious", "Ethno-nationalist", "Single issue")),
sliderInput("freq",
"Minimum Frequency:",
min = 1,  max = 5, value = 1),
sliderInput("max",
"Maximum Number of Words:",
min = 1,  max = 100,  value = 50)
),
# Show Word Cloud
mainPanel(
plotOutput("plot")
)
)
)
server <- function(input, output, session) {
# Make the wordcloud drawing predictable during a session
wordcloud_rep <- repeatable(wordcloud)
output$plot <- renderPlot({
filtered <-
allwords %>%
dplyr::filter(ideology == input$ideology
)
filteredcolour <-
allcolours %>%
dplyr::filter(ideology == input$ideology
)
wordcloud_rep(filtered$term, filtered$n, scale=c(2.5, .4),
min.freq = input$freq, max.words=input$max,
colors=as.character(filteredcolour$colours))
})
}
shinyApp(ui = ui, server = server)
shiny::runApp('test')
library(dplyr)
library(knitr)
library(readxl)
terror <- read_excel("PPT-US_0517dist.xlsx")
#View(terror)
terror <- terror[c("DOM_I","PHIL")]
attach(terror)
terror$IDEO[DOM_I == 1] <- "Extreme Right Wing"
terror$IDEO[DOM_I == 2] <- "Extreme Left Wing"
terror$IDEO[DOM_I == 3] <- "Religious"
terror$IDEO[DOM_I == 4] <- "Ethno-nationalist/Separatist"
terror$IDEO[DOM_I == 5] <- "Single Issue"
terror$IDEO[DOM_I == -99] <- "Uncertain"
detach(terror)
terror = terror[!terror$DOM_I == -99,]
#subset by ideologies
right = terror[terror$DOM_I == 1,]
left = terror[terror$DOM_I == 2,]
relig = terror[terror$DOM_I == 3,]
ethno = terror[terror$DOM_I == 4,]
single = terror[terror$DOM_I == 5,]
# using text analysis
library(tm)
library(stringr)
right_text <- data.frame(doc_id=right$DOM_I, text=right$PHIL, stringsAsFactors = FALSE)
df_source_right <- DataframeSource(right_text)
tm_right <- VCorpus(df_source_right)
# our custom vector of stop words
my_custom_stopwords <- c("group", "philosophical", "movement", "ideological", "philosophically", "philosophy", "philosophies")
tm2_right <- tm_map(tm_right, content_transformer(tolower))
tm2_right <- tm_map(tm2_right, removePunctuation)
tm2_right <- tm_map(tm2_right, removeWords, c(stopwords("en")))
tm2_right <- tm_map(tm2_right, removeWords, my_custom_stopwords)
tm2_right <- tm_map(tm2_right, removeNumbers)
removeNumPunct <- function(x){gsub("[^[:alpha:][:space:]]*", "", x)}
tm2_right <- tm_map(tm2_right, content_transformer(removeNumPunct))
tm2_right <- tm_map(tm2_right, stripWhitespace)
# stemming
library(SnowballC)
# Stem all words
tm_stemmed_right <- tm_map(tm2_right, stemDocument)
# Stem completion
stemCompletion2 <- function(x, dictionary) {
x <- unlist(strsplit(as.character(x), " "))
x <- x[x != ""]
x <- stemCompletion(x, dictionary=dictionary)
x <- paste(x, sep="", collapse=" ")
PlainTextDocument(stripWhitespace(x))
}
tm_all_right <- lapply(tm_stemmed_right, stemCompletion2,
dictionary=tm2_right)
tm_right <- VCorpus(VectorSource(tm_all_right))
library(tm)
dtm_right<-DocumentTermMatrix(tm_right)
library(tidytext)
sotu_td_right <- tidy(dtm_right)
sotu_tf_idf_right <-  sotu_td_right %>%
bind_tf_idf(term, document, count) %>%
arrange(desc(tf_idf))
# top 300 words using count
top_words_right <- sotu_tf_idf_right %>%
dplyr::group_by(term) %>%
dplyr::summarise(n = sum(count)) %>%
dplyr::top_n(n = 300, wt = n)  %>%
dplyr::ungroup() %>%
dplyr::mutate(term = reorder(term, n))
# Load wordcloud package
library(wordcloud)
# using text analysis - left wing
library(tm)
left_text <- data.frame(doc_id=left$DOM_I, text=left$PHIL, stringsAsFactors = FALSE)
df_source_left <- DataframeSource(left_text)
tm_left <- VCorpus(df_source_left)
tm2_left <- tm_map(tm_left, content_transformer(tolower))
tm2_left <- tm_map(tm2_left, removePunctuation)
tm2_left <- tm_map(tm2_left, removeWords, c(stopwords("en")))
tm2_left <- tm_map(tm2_left, removeWords, my_custom_stopwords)
tm2_left <- tm_map(tm2_left, removeNumbers)
removeNumPunct <- function(x){gsub("[^[:alpha:][:space:]]*", "", x)}
tm2_left <- tm_map(tm2_left, content_transformer(removeNumPunct))
tm2_left <- tm_map(tm2_left, stripWhitespace)
# stemming
library(SnowballC)
# Stem all words
tm_stemmed_left <- tm_map(tm2_left, stemDocument)
# Stem completion
stemCompletion2 <- function(x, dictionary) {
x <- unlist(strsplit(as.character(x), " "))
x <- x[x != ""]
x <- stemCompletion(x, dictionary=dictionary)
x <- paste(x, sep="", collapse=" ")
PlainTextDocument(stripWhitespace(x))
}
tm_all_left <- lapply(tm_stemmed_left, stemCompletion2,
dictionary=tm2_left)
tm_left <- VCorpus(VectorSource(tm_all_left))
library(tm)
dtm_left<-DocumentTermMatrix(tm_left)
library(tidytext)
sotu_td_left <- tidy(dtm_left)
sotu_tf_idf_left <-  sotu_td_left %>%
bind_tf_idf(term, document, count) %>%
arrange(desc(tf_idf))
# top 300 words using count
top_words_left <- sotu_tf_idf_left %>%
dplyr::group_by(term) %>%
dplyr::summarise(n = sum(count)) %>%
dplyr::top_n(n = 300, wt = n)  %>%
dplyr::ungroup() %>%
dplyr::mutate(term = reorder(term, n))
# Load wordcloud package
library(wordcloud)
# using text analysis - religious
library(tm)
relig_text <- data.frame(doc_id=relig$DOM_I, text=relig$PHIL, stringsAsFactors = FALSE)
df_source_relig <- DataframeSource(relig_text)
tm_relig <- VCorpus(df_source_relig)
tm2_relig <- tm_map(tm_relig, content_transformer(tolower))
tm2_relig <- tm_map(tm2_relig, removePunctuation)
tm2_relig <- tm_map(tm2_relig, removeWords, c(stopwords("en")))
tm2_relig <- tm_map(tm2_relig, removeWords, my_custom_stopwords)
tm2_relig <- tm_map(tm2_relig, removeNumbers)
removeNumPunct <- function(x){gsub("[^[:alpha:][:space:]]*", "", x)}
tm2_relig <- tm_map(tm2_relig, content_transformer(removeNumPunct))
tm2_relig <- tm_map(tm2_relig, stripWhitespace)
# stemming
library(SnowballC)
# Stem all words
tm_stemmed_relig <- tm_map(tm2_relig, stemDocument)
# Stem completion
stemCompletion2 <- function(x, dictionary) {
x <- unlist(strsplit(as.character(x), " "))
x <- x[x != ""]
x <- stemCompletion(x, dictionary=dictionary)
x <- paste(x, sep="", collapse=" ")
PlainTextDocument(stripWhitespace(x))
}
tm_all_relig <- lapply(tm_stemmed_relig, stemCompletion2,
dictionary=tm2_relig)
tm_relig <- VCorpus(VectorSource(tm_all_relig))
library(tm)
dtm_relig<-DocumentTermMatrix(tm_relig)
library(tidytext)
sotu_td_relig <- tidy(dtm_relig)
sotu_tf_idf_relig <-  sotu_td_relig %>%
bind_tf_idf(term, document, count) %>%
arrange(desc(tf_idf))
# top 300 words using count
top_words_relig <- sotu_tf_idf_relig %>%
dplyr::group_by(term) %>%
dplyr::summarise(n = sum(count)) %>%
dplyr::top_n(n = 300, wt = n)  %>%
dplyr::ungroup() %>%
dplyr::mutate(term = reorder(term, n))
# Load wordcloud package
library(wordcloud)
# using text analysis - ethno
library(tm)
ethno_text <- data.frame(doc_id=ethno$DOM_I, text=ethno$PHIL, stringsAsFactors = FALSE)
df_source_ethno <- DataframeSource(ethno_text)
tm_ethno <- VCorpus(df_source_ethno)
tm2_ethno <- tm_map(tm_ethno, content_transformer(tolower))
tm2_ethno <- tm_map(tm2_ethno, removePunctuation)
tm2_ethno <- tm_map(tm2_ethno, removeWords, c(stopwords("en")))
tm2_ethno <- tm_map(tm2_ethno, removeWords, my_custom_stopwords)
tm2_ethno <- tm_map(tm2_ethno, removeNumbers)
removeNumPunct <- function(x){gsub("[^[:alpha:][:space:]]*", "", x)}
tm2_ethno <- tm_map(tm2_ethno, content_transformer(removeNumPunct))
tm2_ethno <- tm_map(tm2_ethno, stripWhitespace)
# stemming
library(SnowballC)
# Stem all words
tm_stemmed_ethno <- tm_map(tm2_ethno, stemDocument)
# Stem completion
stemCompletion2 <- function(x, dictionary) {
x <- unlist(strsplit(as.character(x), " "))
x <- x[x != ""]
x <- stemCompletion(x, dictionary=dictionary)
x <- paste(x, sep="", collapse=" ")
PlainTextDocument(stripWhitespace(x))
}
tm_all_ethno <- lapply(tm_stemmed_ethno, stemCompletion2,
dictionary=tm2_ethno)
tm_ethno <- VCorpus(VectorSource(tm_all_ethno))
library(tm)
dtm_ethno<-DocumentTermMatrix(tm_ethno)
library(tidytext)
sotu_td_ethno <- tidy(dtm_ethno)
sotu_tf_idf_ethno <-  sotu_td_ethno %>%
bind_tf_idf(term, document, count) %>%
arrange(desc(tf_idf))
# top 300 words using count
top_words_ethno <- sotu_tf_idf_ethno %>%
dplyr::group_by(term) %>%
dplyr::summarise(n = sum(count)) %>%
dplyr::top_n(n = 300, wt = n)  %>%
dplyr::ungroup() %>%
dplyr::mutate(term = reorder(term, n))
# Load wordcloud package
library(wordcloud)
# using text analysis - single
library(tm)
single_text <- data.frame(doc_id=single$DOM_I, text=single$PHIL, stringsAsFactors = FALSE)
df_source_single <- DataframeSource(single_text)
tm_single <- VCorpus(df_source_single)
tm2_single <- tm_map(tm_single, content_transformer(tolower))
tm2_single <- tm_map(tm2_single, removePunctuation)
tm2_single <- tm_map(tm2_single, removeWords, c(stopwords("en")))
tm2_single <- tm_map(tm2_single, removeWords, my_custom_stopwords)
tm2_single <- tm_map(tm2_single, removeNumbers)
removeNumPunct <- function(x){gsub("[^[:alpha:][:space:]]*", "", x)}
tm2_single <- tm_map(tm2_single, content_transformer(removeNumPunct))
tm2_single <- tm_map(tm2_single, stripWhitespace)
# stemming
library(SnowballC)
# Stem all words
tm_stemmed_single <- tm_map(tm2_single, stemDocument)
# Stem completion
stemCompletion2 <- function(x, dictionary) {
x <- unlist(strsplit(as.character(x), " "))
x <- x[x != ""]
x <- stemCompletion(x, dictionary=dictionary)
x <- paste(x, sep="", collapse=" ")
PlainTextDocument(stripWhitespace(x))
}
tm_all_single <- lapply(tm_stemmed_single, stemCompletion2,
dictionary=tm2_single)
tm_single <- VCorpus(VectorSource(tm_all_single))
library(tm)
dtm_single<-DocumentTermMatrix(tm_single)
library(tidytext)
sotu_td_single <- tidy(dtm_single)
sotu_tf_idf_single <-  sotu_td_single %>%
bind_tf_idf(term, document, count) %>%
arrange(desc(tf_idf))
# top 300 words using count
top_words_single <- sotu_tf_idf_single %>%
dplyr::group_by(term) %>%
dplyr::summarise(n = sum(count)) %>%
dplyr::top_n(n = 300, wt = n)  %>%
dplyr::ungroup() %>%
dplyr::mutate(term = reorder(term, n))
top_words_right <- merge(c("Right-wing"), top_words_right)
top_words_left <- merge(c("Left-wing"), top_words_left)
top_words_relig <- merge(c("Religious"), top_words_relig)
top_words_ethno <- merge(c("Ethno-nationalist"), top_words_ethno)
top_words_single <- merge(c("Single issue"), top_words_single)
allwords <- rbind(top_words_right, top_words_left)
allwords <- rbind(allwords, top_words_relig)
allwords <- rbind(allwords, top_words_ethno)
allwords <- rbind(allwords, top_words_single)
names(allwords)[1] <- "ideology"
blue_gradient <- brewer.pal(9, "Blues")
blue_gradient <- blue_gradient[-(7:9)]
red_gradient <- brewer.pal(9, "Reds")
red_gradient <- red_gradient[-(7:9)]
green_gradient <- brewer.pal(9, "Greens")
green_gradient <- green_gradient[-(7:9)]
purple_gradient <- brewer.pal(9, "Purples")
purple_gradient <- purple_gradient[-(7:9)]
orange_gradient <- brewer.pal(9, "Oranges")
orange_gradient <- orange_gradient[-(7:9)]
blue_gradient <- merge(c("Right-wing"), blue_gradient)
red_gradient <- merge(c("Left-wing"), red_gradient)
green_gradient <- merge(c("Religious"), green_gradient)
purple_gradient <- merge(c("Ethno-nationalist"), purple_gradient)
orange_gradient <- merge(c("Single issue"), orange_gradient)
allcolours <- rbind(blue_gradient, red_gradient)
allcolours <- rbind(allcolours, green_gradient)
allcolours <- rbind(allcolours, purple_gradient)
allcolours <- rbind(allcolours, orange_gradient)
names(allcolours)[1] <- "ideology"
names(allcolours)[2] <- "colours"
require(shiny)
inputPanel(
# Sidebar with a slider and selection inputs
sidebarPanel(
selectInput("ideology", "Choose an ideology:",
choices = c("Right-wing", "Left-wing", "Religious", "Ethno-nationalist", "Single issue")),
sliderInput("freq",
"Minimum Frequency:",
min = 1,  max = 5, value = 1),
sliderInput("max",
"Maximum Number of Words:",
min = 1,  max = 100,  value = 50)
)
)
wordcloud_rep <- repeatable(wordcloud)
renderPlot ({
filtered <-
allwords %>%
dplyr::filter(ideology == input$ideology
)
filteredcolour <-
allcolours %>%
dplyr::filter(ideology == input$ideology
)
wordcloud_rep(filtered$term, filtered$n, scale=c(2.5, .4),
min.freq = input$freq, max.words=input$max,
colors=as.character(filteredcolour$colours))
})
